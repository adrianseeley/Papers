# Exploring Latent Classifications in Self-Reported Fire-Setting Behaviors

Adrian Seeley (OCT 2024)

- When we create a cluster seed, we are choosing 1 datapoint to serve as the anchor point for that cluster, however a problem occurs when we attempt to add the second point. For example lets say we are creating three clusters, and we have a seed point in each one. The average distance between points for all clusters is currently zero. When deciding where to place the first point, we are aiming to minimize the total increase in average distances between points in clusters. This first addition will cause one of the clusters to enter a non-zero average distance state. Then when we go to add a second point does a bias get created to maintain a zero average distance state? 
- Okay, this is proven with experimentation that they all cling to a single cluster in a biased way. This is a little unfortunate since to use this method without introducing a penalty component we actually need to seed each cluster with 2 datapoints to lay the basis for an average distance between points. This means that even with 2 cluster we need 4 seed datapoints, which means there are N choose K*2 seeds instead of N choose K. Thats exceedinly punitive as we increase the K value, but still tractable, lets implement and confirm the bias removal. Note that 162 choose 4 (for 2 clusters) is 27 646 920 unique combinations that need to be created.
- Confirmed that this does remove the issue of a single biased cluster that absorbs all points, but it magnitudinally increases calculation time, which was already quite heavy. Though even as a single threaded operation it is quite tractable for the 200 datapoint range and a handful of clusters (probably < 5)
- 200 choose 6 (aka 3 clusters) is 82 408 626 300, which is 82 billion, is heavy but well within reason. 
- 200 choose 8 (aka 4 clusters) is 55 098 996 177 225, which is 55 trillion is definitely pushing the boundaries of what is possible with this method without some type of large distribution, though each seed could easily be isolated to its own thread and parallellized. On a 128 core machine thats roughly 55 098 996 177 225 / 128 = 430 460 907 635, or 430 billion clusterings per core. This would also be an excellent cuda candidate being that the dataset is small, readonly, and each thread simply needs an index to expand its iterator, or the iterator can be provided wholesale.
- Another similar method could be to evenly distribute all points to a cluster randomly then make continuous sweeps where each datapoints tries to migrate to a different cluster to improve the global average distance within clusters, and continues until no more improvements are possible. It's a bit of a TSP.
- If we do a standard kmeans or gmm/bmm and compare the resultant inner cluster distances and makeup we might show that there is an improvement to be had here.
- Interestingly because the weighted average distance within clusters is agnostic to the number of clusters and provides a metric that remains meaningful across different cluster counts. This means we could in theory create different cluster sizes then find the lowest WAD overall, though I am curious if the metric will tend towards clusters of 2 datapoints? This will need to be tested, or at least tried since an exhaustive exploration of a single cluster size is already intractable.
- There does seem to be local minima between cluster migrations that are effectively inescapable in this setup, but I suspect a shuffle and average of the final WAD will reveal a sort of difficulty in improving beyond a certain point, I'll try that next.
- Okay so there is little wiggle around the average even on the max side over many trials, however there also appears to be a gradual and continuous return on gains for adding extra clusters
- So there are other measures of quality, but I think ultimately the parsimonious nature of the classifications are up to human discretion, so the next goal is to create smears of the different clusters to see what visually comes out in the clustering or if its basically random.
- The smears look interesting, I think I need to interal sort clusters by similarity greedily to present them in the best way so I'll give that a shot.
- Id be curious to see the odds ratio of zero high risk vs one or more high risk groups